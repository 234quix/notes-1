<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Adding Dropout" />
<meta property="og:description" content="How to add dropout to a neural networking for deep learning in Python.." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chrisalbon.com/deep_learning_keras/adding_dropout/" />



<meta property="article:published_time" content="2017-12-20T11:53:49-07:00"/>

<meta property="article:modified_time" content="2017-12-20T11:53:49-07:00"/>











<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Adding Dropout"/>
<meta name="twitter:description" content="How to add dropout to a neural networking for deep learning in Python.."/>
<meta name="generator" content="Hugo 0.31.1" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Adding Dropout",
  "url": "https://chrisalbon.com/deep_learning_keras/adding_dropout/",
  "wordCount": "333",
  "datePublished": "2017-12-20T11:53:49-07:00",
  "dateModified": "2017-12-20T11:53:49-07:00",
  "author": {
    "@type": "Person",
    "name": "Chris Albon"
  },
  "keywords": "deep_learning_keras, Basics",
  "description": "How to add dropout to a neural networking for deep learning in Python.."
}
</script>



    <link rel="canonical" href="https://chrisalbon.com/deep_learning_keras/adding_dropout/">

    <title>Adding Dropout | Chris Albon</title>

    <!-- combined, minified CSS -->
    <link href="https://chrisalbon.com/css/style.css" rel="stylesheet" integrity="" crossorigin="anonymous">
    <link href="https://chrisalbon.com/css/main.css" rel="stylesheet" integrity="" crossorigin="anonymous">

    

    

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

  </head>

  <body>

    <div class="blog-masthead">
      <div class="container">
        <nav class="nav blog-nav">
          <a class="nav-link" href="https://chrisalbon.com/">Home</a></nav>
      </div>
    </div>

    <div class="container">
      <div class="row">
        <div class="col-sm-12 blog-main">

          


<article class="blog-post">
  <header>
    <h2 class="blog-post-title"><a href="https://chrisalbon.com/deep_learning_keras/adding_dropout/">Adding Dropout</a></h2>
    <p class="blog-post-meta"><time datetime="2017-12-20T11:53:49-07:00">Wed Dec 20, 2017</time> in 
<i class="fa fa-folder" aria-hidden="true"></i>&nbsp;<a href="/categories/deep_learning_keras" rel="category tag">deep_learning_keras</a>


<i class="fa fa-tag" aria-hidden="true"></i>&nbsp;<a href="/tags/basics" rel="tag">Basics</a>

</p>
  </header>
  

<p><a alt="Dropout" href="https://machinelearningflashcards.com">
    <img src="adding_dropout/Dropout_print.png" class="flashcard center-block">
</a></p>

<h2 id="preliminaries">Preliminaries</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Load libraries</span>
<span style="color:#080;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">numpy</span> <span style="color:#080;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">np</span>
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">keras.datasets</span> <span style="color:#080;font-weight:bold">import</span> imdb
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">keras.preprocessing.text</span> <span style="color:#080;font-weight:bold">import</span> Tokenizer
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">keras</span> <span style="color:#080;font-weight:bold">import</span> models
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">keras</span> <span style="color:#080;font-weight:bold">import</span> layers

<span style="color:#888"># Set random seed</span>
np<span style="color:#333">.</span>random<span style="color:#333">.</span>seed(<span style="color:#00d;font-weight:bold">0</span>)</code></pre></div>
<pre><code>Using TensorFlow backend.
</code></pre>

<h2 id="load-imdb-movie-review-data">Load IMDB Movie Review Data</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Set the number of features we want</span>
number_of_features <span style="color:#333">=</span> <span style="color:#00d;font-weight:bold">1000</span>

<span style="color:#888"># Load data and target vector from movie review data</span>
(train_data, train_target), (test_data, test_target) <span style="color:#333">=</span> imdb<span style="color:#333">.</span>load_data(num_words<span style="color:#333">=</span>number_of_features)

<span style="color:#888"># Convert movie review data to a one-hot encoded feature matrix</span>
tokenizer <span style="color:#333">=</span> Tokenizer(num_words<span style="color:#333">=</span>number_of_features)
train_features <span style="color:#333">=</span> tokenizer<span style="color:#333">.</span>sequences_to_matrix(train_data, mode<span style="color:#333">=</span><span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;binary&#39;</span>)
test_features <span style="color:#333">=</span> tokenizer<span style="color:#333">.</span>sequences_to_matrix(test_data, mode<span style="color:#333">=</span><span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;binary&#39;</span>)</code></pre></div>
<h2 id="construct-neural-network-architecture-with-dropout-layer">Construct Neural Network Architecture With Dropout Layer</h2>

<p>In Keras, we can implement dropout by added <code>Dropout</code> layers into our network architecture. Each <code>Dropout</code> layer will drop a user-defined hyperparameter of units in the previous layer every batch. Remember in Keras the input layer is assumed to be the first layer and not added using the <code>add</code>. Therefore, if we want to add dropout to the input layer, the layer we add in our is a dropout layer. This layer contains both the proportion of the input layer&rsquo;s units to drop <code>0.2</code> and <code>input_shape</code> defining the shape of the observation data. Next, after we add a dropout layer with <code>0.5</code> after each of the hidden layers.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Start neural network</span>
network <span style="color:#333">=</span> models<span style="color:#333">.</span>Sequential()

<span style="color:#888"># Add a dropout layer for input layer</span>
network<span style="color:#333">.</span>add(layers<span style="color:#333">.</span>Dropout(<span style="color:#60e;font-weight:bold">0.2</span>, input_shape<span style="color:#333">=</span>(number_of_features,)))

<span style="color:#888"># Add fully connected layer with a ReLU activation function</span>
network<span style="color:#333">.</span>add(layers<span style="color:#333">.</span>Dense(units<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">16</span>, activation<span style="color:#333">=</span><span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;relu&#39;</span>))

<span style="color:#888"># Add a dropout layer for previous hidden layer</span>
network<span style="color:#333">.</span>add(layers<span style="color:#333">.</span>Dropout(<span style="color:#60e;font-weight:bold">0.5</span>))

<span style="color:#888"># Add fully connected layer with a ReLU activation function</span>
network<span style="color:#333">.</span>add(layers<span style="color:#333">.</span>Dense(units<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">16</span>, activation<span style="color:#333">=</span><span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;relu&#39;</span>))

<span style="color:#888"># Add a dropout layer for previous hidden layer</span>
network<span style="color:#333">.</span>add(layers<span style="color:#333">.</span>Dropout(<span style="color:#60e;font-weight:bold">0.5</span>))

<span style="color:#888"># Add fully connected layer with a sigmoid activation function</span>
network<span style="color:#333">.</span>add(layers<span style="color:#333">.</span>Dense(units<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">1</span>, activation<span style="color:#333">=</span><span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;sigmoid&#39;</span>))</code></pre></div>
<h2 id="compile-neural-network">Compile Neural Network</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Compile neural network</span>
network<span style="color:#333">.</span><span style="color:#007020">compile</span>(loss<span style="color:#333">=</span><span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;binary_crossentropy&#39;</span>, <span style="color:#888"># Cross-entropy</span>
                optimizer<span style="color:#333">=</span><span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;rmsprop&#39;</span>, <span style="color:#888"># Root Mean Square Propagation</span>
                metrics<span style="color:#333">=</span>[<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;accuracy&#39;</span>]) <span style="color:#888"># Accuracy performance metric</span></code></pre></div>
<h2 id="train-neural-network">Train Neural Network</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Train neural network</span>
history <span style="color:#333">=</span> network<span style="color:#333">.</span>fit(train_features, <span style="color:#888"># Features</span>
                      train_target, <span style="color:#888"># Target vector</span>
                      epochs<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">3</span>, <span style="color:#888"># Number of epochs</span>
                      verbose<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">0</span>, <span style="color:#888"># No output</span>
                      batch_size<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">100</span>, <span style="color:#888"># Number of observations per batch</span>
                      validation_data<span style="color:#333">=</span>(test_features, test_target)) <span style="color:#888"># Data for evaluation</span></code></pre></div>

</article> 



        </div> <!-- /.blog-main -->

      </div> <!-- /.row -->
    </div> <!-- /.container -->

    <footer class="blog-footer">
      <p>
      
      Blog template created by <a href="https://twitter.com/mdo">@mdo</a>, ported to Hugo by <a href='https://twitter.com/mralanorth'>@mralanorth</a>.
      
      </p>
    </footer>

  </body>

</html>
