<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="K-Nearest Neighbors Classification" />
<meta property="og:description" content="A quick guide to using k-nearest neighbor using numpy and scikit." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chrisalbon.com/machine_learning/nearest_neighbors/k-nearest_neighbors_classifer/" />



<meta property="article:published_time" content="2017-12-20T11:53:49-07:00"/>

<meta property="article:modified_time" content="2017-12-20T11:53:49-07:00"/>











<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="K-Nearest Neighbors Classification"/>
<meta name="twitter:description" content="A quick guide to using k-nearest neighbor using numpy and scikit."/>
<meta name="generator" content="Hugo 0.31.1" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "K-Nearest Neighbors Classification",
  "url": "https://chrisalbon.com/machine_learning/nearest_neighbors/k-nearest_neighbors_classifer/",
  "wordCount": "626",
  "datePublished": "2017-12-20T11:53:49-07:00",
  "dateModified": "2017-12-20T11:53:49-07:00",
  "author": {
    "@type": "Person",
    "name": "Chris Albon"
  },
  "description": "A quick guide to using k-nearest neighbor using numpy and scikit."
}
</script>



    <link rel="canonical" href="https://chrisalbon.com/machine_learning/nearest_neighbors/k-nearest_neighbors_classifer/">

    <title>K-Nearest Neighbors Classification | Chris Albon</title>

    <!-- combined, minified CSS -->
    <link href="https://chrisalbon.com/css/style.css" rel="stylesheet" integrity="" crossorigin="anonymous">
    <link href="https://chrisalbon.com/css/main.css" rel="stylesheet" integrity="" crossorigin="anonymous">

    

    

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

  </head>

  <body>

    <div class="blog-masthead">
      <div class="container">
        <nav class="nav blog-nav">
          <a class="nav-link" href="https://chrisalbon.com/">Home</a></nav>
      </div>
    </div>

    <div class="container">
      <div class="row">
        <div class="col-sm-12 blog-main">

          


<article class="blog-post">
  <header>
    <h2 class="blog-post-title"><a href="https://chrisalbon.com/machine_learning/nearest_neighbors/k-nearest_neighbors_classifer/">K-Nearest Neighbors Classification</a></h2>
    <p class="blog-post-meta"><time datetime="2017-12-20T11:53:49-07:00">Wed Dec 20, 2017</time></p>
  </header>
  

<p>K-nearest neighbors classifier (KNN) is a simple and powerful classification learner.</p>

<p>KNN has three basic parts:</p>

<ul>
<li>$y_i$: The class of an observation (what we are trying to predict in the test data).</li>
<li>$X_i$: The predictors/IVs/attributes of an observation.</li>
<li>$K$: A positive number specified by the researcher. K denotes the number of observations closest to a particular observation that define its &ldquo;neighborhood&rdquo;. For example, K=2 means that each observation&rsquo;s has a neighorhood comprising of the two other observations closest to it.</li>
</ul>

<p>Imagine we have an observation where we know its independent variables $x<em>{test}$ but do not know its class $y</em>{test}$. The KNN learner finds the K other observations that are closest to $x<em>{test}$ and uses their known classes to assign a classes to $x</em>{test}$.</p>

<h2 id="preliminaries">Preliminaries</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">pandas</span> <span style="color:#080;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">pd</span>
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">sklearn</span> <span style="color:#080;font-weight:bold">import</span> neighbors
<span style="color:#080;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">numpy</span> <span style="color:#080;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">np</span>
<span style="color:#333">%</span>matplotlib inline  
<span style="color:#080;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">seaborn</span></code></pre></div>
<h2 id="create-dataset">Create Dataset</h2>

<p>Here we create three variables, <code>test_1</code> and <code>test_2</code> are our independent variables, &lsquo;outcome&rsquo; is our dependent variable. We will use this data to train our learner.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">training_data <span style="color:#333">=</span> pd<span style="color:#333">.</span>DataFrame()

training_data[<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;test_1&#39;</span>] <span style="color:#333">=</span> [<span style="color:#60e;font-weight:bold">0.3051</span>,<span style="color:#60e;font-weight:bold">0.4949</span>,<span style="color:#60e;font-weight:bold">0.6974</span>,<span style="color:#60e;font-weight:bold">0.3769</span>,<span style="color:#60e;font-weight:bold">0.2231</span>,<span style="color:#60e;font-weight:bold">0.341</span>,<span style="color:#60e;font-weight:bold">0.4436</span>,<span style="color:#60e;font-weight:bold">0.5897</span>,<span style="color:#60e;font-weight:bold">0.6308</span>,<span style="color:#60e;font-weight:bold">0.5</span>]
training_data[<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;test_2&#39;</span>] <span style="color:#333">=</span> [<span style="color:#60e;font-weight:bold">0.5846</span>,<span style="color:#60e;font-weight:bold">0.2654</span>,<span style="color:#60e;font-weight:bold">0.2615</span>,<span style="color:#60e;font-weight:bold">0.4538</span>,<span style="color:#60e;font-weight:bold">0.4615</span>,<span style="color:#60e;font-weight:bold">0.8308</span>,<span style="color:#60e;font-weight:bold">0.4962</span>,<span style="color:#60e;font-weight:bold">0.3269</span>,<span style="color:#60e;font-weight:bold">0.5346</span>,<span style="color:#60e;font-weight:bold">0.6731</span>]
training_data[<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;outcome&#39;</span>] <span style="color:#333">=</span> [<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;win&#39;</span>,<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;win&#39;</span>,<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;win&#39;</span>,<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;win&#39;</span>,<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;win&#39;</span>,<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;loss&#39;</span>,<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;loss&#39;</span>,<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;loss&#39;</span>,<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;loss&#39;</span>,<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;loss&#39;</span>]

training_data<span style="color:#333">.</span>head()</code></pre></div>
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>test_1</th>
      <th>test_2</th>
      <th>outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.3051</td>
      <td>0.5846</td>
      <td>win</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.4949</td>
      <td>0.2654</td>
      <td>win</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.6974</td>
      <td>0.2615</td>
      <td>win</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.3769</td>
      <td>0.4538</td>
      <td>win</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.2231</td>
      <td>0.4615</td>
      <td>win</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="plot-the-data">Plot the data</h2>

<p>This is not necessary, but because we only have three variables, we can plot the training dataset. The X and Y axes are the independent variables, while the colors of the points are their classes.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">seaborn<span style="color:#333">.</span>lmplot(<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;test_1&#39;</span>, <span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;test_2&#39;</span>, data<span style="color:#333">=</span>training_data, fit_reg<span style="color:#333">=</span>False,hue<span style="color:#333">=</span><span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#34;outcome&#34;</span>, scatter_kws<span style="color:#333">=</span>{<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#34;marker&#34;</span>: <span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#34;D&#34;</span>,<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#34;s&#34;</span>: <span style="color:#00d;font-weight:bold">100</span>})</code></pre></div>
<pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x11008aeb8&gt;
</code></pre>

<p><img src="k-nearest_neighbors_classifer_9_1.png" alt="png" /></p>

<h2 id="convert-data-into-np-arrays">Convert Data Into np.arrays</h2>

<p>The <code>scikit-learn</code> library requires the data be formatted as a <code>numpy</code> array. Here are doing that reformatting.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">X <span style="color:#333">=</span> training_data<span style="color:#333">.</span>as_matrix(columns<span style="color:#333">=</span>[<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;test_1&#39;</span>, <span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;test_2&#39;</span>])
y <span style="color:#333">=</span> np<span style="color:#333">.</span>array(training_data[<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;outcome&#39;</span>])</code></pre></div>
<h2 id="train-the-learner">Train The Learner</h2>

<p>This is our big moment. We train a KNN learner using the parameters that an observation&rsquo;s neighborhood is its three closest neighors. <code>weights = 'uniform'</code> can be thought of as the voting system used. For example, <code>uniform</code> means that all neighbors get an equally weighted &ldquo;vote&rdquo; about an observation&rsquo;s class while <code>weights = 'distance'</code> would tell the learner to weigh each observation&rsquo;s &ldquo;vote&rdquo; by its distance from the observation we are classifying.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">clf <span style="color:#333">=</span> neighbors<span style="color:#333">.</span>KNeighborsClassifier(<span style="color:#00d;font-weight:bold">3</span>, weights <span style="color:#333">=</span> <span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;uniform&#39;</span>)
trained_model <span style="color:#333">=</span> clf<span style="color:#333">.</span>fit(X, y)</code></pre></div>
<h2 id="view-the-model-s-score">View The Model&rsquo;s Score</h2>

<p>How good is our trained model compared to our training data?</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">trained_model<span style="color:#333">.</span>score(X, y)</code></pre></div>
<pre><code>0.80000000000000004
</code></pre>

<p>Our model is 80% accurate!</p>

<p><em>Note: that in any real world example we&rsquo;d want to compare the trained model to some holdout test data. But since this is a toy example I used the training data</em>.</p>

<h2 id="apply-the-learner-to-a-new-data-point">Apply The Learner To A New Data Point</h2>

<p>Now that we have trained our model, we can predict the class any new observation, $y_{test}$. Let us do that now!</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Create a new observation with the value of the first independent variable, &#39;test_1&#39;, as .4 </span>
<span style="color:#888"># and the second independent variable, test_1&#39;, as .6 </span>
x_test <span style="color:#333">=</span> np<span style="color:#333">.</span>array([[<span style="color:#333">.</span><span style="color:#00d;font-weight:bold">4</span>,<span style="color:#333">.</span><span style="color:#00d;font-weight:bold">6</span>]])</code></pre></div><div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Apply the learner to the new, unclassified observation.</span>
trained_model<span style="color:#333">.</span>predict(x_test)</code></pre></div>
<pre><code>array(['loss'], dtype=object)
</code></pre>

<p>Huzzah! We can see that the learner has predicted that the new observation&rsquo;s class is <code>loss</code>.</p>

<p>We can even look at the probabilities the learner assigned to each class:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">trained_model<span style="color:#333">.</span>predict_proba(x_test)</code></pre></div>
<pre><code>array([[ 0.66666667,  0.33333333]])
</code></pre>

<p>According to this result, the model predicted that the observation was <code>loss</code> with a ~67% probability and <code>win</code> with a ~33% probability. Because the observation had a greater probability of being <code>loss</code>, it predicted that class for the observation.</p>

<h2 id="notes">Notes</h2>

<ul>
<li>The choice of K has major affects on the classifer created.</li>
<li>The greater the K, more linear (high bias and low variance) the decision boundary.</li>
<li>There are a variety of ways to measure distance, two popular being simple euclidean distance and cosine similarity.</li>
</ul>


</article> 



        </div> <!-- /.blog-main -->

      </div> <!-- /.row -->
    </div> <!-- /.container -->

    <footer class="blog-footer">
      <p>
      
      Blog template created by <a href="https://twitter.com/mdo">@mdo</a>, ported to Hugo by <a href='https://twitter.com/mralanorth'>@mralanorth</a>.
      
      </p>
    </footer>

  </body>

</html>
