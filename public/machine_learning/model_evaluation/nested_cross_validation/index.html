<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Nested Cross Validation" />
<meta property="og:description" content="Nested Cross Validation using scikit-learn." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chrisalbon.com/machine_learning/model_evaluation/nested_cross_validation/" />



<meta property="article:published_time" content="2017-12-20T11:53:49-07:00"/>

<meta property="article:modified_time" content="2017-12-20T11:53:49-07:00"/>











<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Nested Cross Validation"/>
<meta name="twitter:description" content="Nested Cross Validation using scikit-learn."/>
<meta name="generator" content="Hugo 0.31.1" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Nested Cross Validation",
  "url": "https://chrisalbon.com/machine_learning/model_evaluation/nested_cross_validation/",
  "wordCount": "569",
  "datePublished": "2017-12-20T11:53:49-07:00",
  "dateModified": "2017-12-20T11:53:49-07:00",
  "author": {
    "@type": "Person",
    "name": "Chris Albon"
  },
  "description": "Nested Cross Validation using scikit-learn."
}
</script>



    <link rel="canonical" href="https://chrisalbon.com/machine_learning/model_evaluation/nested_cross_validation/">

    <title>Nested Cross Validation | Chris Albon</title>

    <!-- combined, minified CSS -->
    <link href="https://chrisalbon.com/css/style.css" rel="stylesheet" integrity="" crossorigin="anonymous">
    <link href="https://chrisalbon.com/css/main.css" rel="stylesheet" integrity="" crossorigin="anonymous">

    

    

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

  </head>

  <body>

    <div class="blog-masthead">
      <div class="container">
        <nav class="nav blog-nav">
          <a class="nav-link" href="https://chrisalbon.com/">Home</a></nav>
      </div>
    </div>

    <div class="container">
      <div class="row">
        <div class="col-sm-12 blog-main">

          


<article class="blog-post">
  <header>
    <h2 class="blog-post-title"><a href="https://chrisalbon.com/machine_learning/model_evaluation/nested_cross_validation/">Nested Cross Validation</a></h2>
    <p class="blog-post-meta"><time datetime="2017-12-20T11:53:49-07:00">Wed Dec 20, 2017</time></p>
  </header>
  

<p>Often we want to tune the parameters of a model (for example, <code>C</code> in a support vector machine). That is, we want to find the value of a parameter that minimizes our loss function. The best way to do this is cross validation:</p>

<ol>
<li>Set the parameter you want to tune to some value.</li>
<li>Split your data into K &lsquo;folds&rsquo; (sections).</li>
<li>Train your model using K-1 folds using the parameter value.</li>
<li>Test your model on the remaining fold.</li>
<li>Repeat steps 3 and 4 so that every fold is the test data once.</li>
<li>Repeat steps 1 to 5 for every possible value of the parameter.</li>
<li>Report the parameter that produced the best result.</li>
</ol>

<p>However, as <a href="http://jmlr.org/papers/volume11/cawley10a/cawley10a.pdf">Cawley and Talbot</a> point out in their 2010 paper, since we used the test set to <em>both</em> select the values of the parameter <em>and</em> evaluate the model, we risk optimistically biasing our model evaluations. For this reason, if a test set is used to select model parameters, then we need a <em>different</em> test set to get an unbiased evaluation of that selected model.</p>

<p>One way to overcome this problem is to have nested cross validations. First, an inner cross validation is used to tune the parameters and select the best model. Second, an outer cross validation is used to evaluate the model selected by the inner cross validation.</p>

<h2 id="preliminaries">Preliminaries</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Load required packages</span>
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">sklearn</span> <span style="color:#080;font-weight:bold">import</span> datasets
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">sklearn.model_selection</span> <span style="color:#080;font-weight:bold">import</span> GridSearchCV, cross_val_score
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">sklearn.preprocessing</span> <span style="color:#080;font-weight:bold">import</span> StandardScaler
<span style="color:#080;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">numpy</span> <span style="color:#080;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">np</span>
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">sklearn.svm</span> <span style="color:#080;font-weight:bold">import</span> SVC</code></pre></div>
<h2 id="get-data">Get Data</h2>

<p>The data for this tutorial is beast cancer data with <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html">30 features and a binary target variable</a>.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Load the data</span>
dataset <span style="color:#333">=</span> datasets<span style="color:#333">.</span>load_breast_cancer()

<span style="color:#888"># Create X from the features</span>
X <span style="color:#333">=</span> dataset<span style="color:#333">.</span>data

<span style="color:#888"># Create y from the target</span>
y <span style="color:#333">=</span> dataset<span style="color:#333">.</span>target</code></pre></div>
<h2 id="standardize-data">Standardize Data</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Create a scaler object</span>
sc <span style="color:#333">=</span> StandardScaler()

<span style="color:#888"># Fit the scaler to the feature data and transform</span>
X_std <span style="color:#333">=</span> sc<span style="color:#333">.</span>fit_transform(X)</code></pre></div>
<h2 id="create-inner-cross-validation-for-parameter-tuning">Create Inner Cross Validation (For Parameter Tuning)</h2>

<p>This is our inner cross validation. We will use this to hunt for the best parameters for <code>C</code>, the penalty for misclassifying a data point. <code>GridSearchCV</code> will conduct steps 1-6 listed at the top of this tutorial.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Create a list of 10 candidate values for the C parameter</span>
C_candidates <span style="color:#333">=</span> <span style="color:#007020">dict</span>(C<span style="color:#333">=</span>np<span style="color:#333">.</span>logspace(<span style="color:#333">-</span><span style="color:#00d;font-weight:bold">4</span>, <span style="color:#00d;font-weight:bold">4</span>, <span style="color:#00d;font-weight:bold">10</span>))

<span style="color:#888"># Create a gridsearch object with the support vector classifier and the C value candidates</span>
clf <span style="color:#333">=</span> GridSearchCV(estimator<span style="color:#333">=</span>SVC(), param_grid<span style="color:#333">=</span>C_candidates)</code></pre></div>
<p>The code below isn&rsquo;t necessary for parameter tuning using nested cross validation, however to demonstrate that our inner cross validation grid search can find the best value for the parameter <code>C</code>, we will run it once here:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Fit the cross validated grid search on the data </span>
clf<span style="color:#333">.</span>fit(X_std, y)

<span style="color:#888"># Show the best value for C</span>
clf<span style="color:#333">.</span>best_estimator_<span style="color:#333">.</span>C</code></pre></div>
<pre><code>2.7825594022071258
</code></pre>

<h2 id="create-outer-cross-validation-for-model-evaluation">Create Outer Cross Validation (For Model Evaluation)</h2>

<p>With our inner cross validation constructed, we can use <code>cross_val_score</code> to evaluate the model with a second (outer) cross validation.</p>

<p>The code below splits the data into three folds, running the inner cross validation on two of the folds (merged together) and then evaluating the model on the third fold. This is repeated three times so that every fold is used for testing once.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">cross_val_score(clf, X_std, y)</code></pre></div>
<pre><code>array([ 0.94736842,  0.97894737,  0.98412698])
</code></pre>

<p>Each the values above is an unbiased evaluation of the model&rsquo;s accuracy, once for each of the three test folds. Averaged together, they would represent the average accuracy of the model found in the inner cross validated grid search.</p>


</article> 



        </div> <!-- /.blog-main -->

      </div> <!-- /.row -->
    </div> <!-- /.container -->

    <footer class="blog-footer">
      <p>
      
      Blog template created by <a href="https://twitter.com/mdo">@mdo</a>, ported to Hugo by <a href='https://twitter.com/mralanorth'>@mralanorth</a>.
      
      </p>
    </footer>

  </body>

</html>
