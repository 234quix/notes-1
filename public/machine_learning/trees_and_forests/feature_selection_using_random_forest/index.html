<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Feature Selection Using Random Forest" />
<meta property="og:description" content="Feature Selection Using Random Forest with scikit-learn." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chrisalbon.com/machine_learning/trees_and_forests/feature_selection_using_random_forest/" />



<meta property="article:published_time" content="2017-12-20T11:53:49-07:00"/>

<meta property="article:modified_time" content="2017-12-20T11:53:49-07:00"/>











<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Feature Selection Using Random Forest"/>
<meta name="twitter:description" content="Feature Selection Using Random Forest with scikit-learn."/>
<meta name="generator" content="Hugo 0.31.1" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Feature Selection Using Random Forest",
  "url": "https://chrisalbon.com/machine_learning/trees_and_forests/feature_selection_using_random_forest/",
  "wordCount": "963",
  "datePublished": "2017-12-20T11:53:49-07:00",
  "dateModified": "2017-12-20T11:53:49-07:00",
  "author": {
    "@type": "Person",
    "name": "Chris Albon"
  },
  "description": "Feature Selection Using Random Forest with scikit-learn."
}
</script>



    <link rel="canonical" href="https://chrisalbon.com/machine_learning/trees_and_forests/feature_selection_using_random_forest/">

    <title>Feature Selection Using Random Forest | Chris Albon</title>

    <!-- combined, minified CSS -->
    <link href="https://chrisalbon.com/css/style.css" rel="stylesheet" integrity="" crossorigin="anonymous">
    <link href="https://chrisalbon.com/css/main.css" rel="stylesheet" integrity="" crossorigin="anonymous">

    

    

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

  </head>

  <body>

    <div class="blog-masthead">
      <div class="container">
        <nav class="nav blog-nav">
          <a class="nav-link" href="https://chrisalbon.com/">Home</a></nav>
      </div>
    </div>

    <div class="container">
      <div class="row">
        <div class="col-sm-12 blog-main">

          


<article class="blog-post">
  <header>
    <h2 class="blog-post-title"><a href="https://chrisalbon.com/machine_learning/trees_and_forests/feature_selection_using_random_forest/">Feature Selection Using Random Forest</a></h2>
    <p class="blog-post-meta"><time datetime="2017-12-20T11:53:49-07:00">Wed Dec 20, 2017</time></p>
  </header>
  

<p>Often in data science we have hundreds or even millions of features and we want a way to create a model that only includes the most important features. This has three benefits. First, we make our model more simple to interpret. Second, we can reduce the variance of the model, and therefore overfitting. Finally, we can reduce the computational cost (and time) of training a model. The process of identifying only the most relevant features is called &ldquo;feature selection.&rdquo;</p>

<p>Random Forests are often used for feature selection in a data science workflow. The reason is because the tree-based strategies used by random forests naturally ranks by how well they improve the purity of the node. This mean decrease in impurity over all trees (called <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity">gini impurity</a>). Nodes with the greatest decrease in impurity happen at the start of the trees, while notes with the least decrease in impurity occur at the end of trees. Thus, by pruning trees below a particular node, we can create a subset of the most important features.</p>

<p>In this tutorial we will:</p>

<ol>
<li>Prepare the dataset</li>
<li>Train a random forest classifier</li>
<li>Identify the most important features</li>
<li>Create a new &lsquo;limited featured&rsquo; dataset containing only those features</li>
<li>Train a second classifier on this new dataset</li>
<li>Compare the accuracy of the &lsquo;full featured&rsquo; classifier to the accuracy of the &lsquo;limited featured&rsquo; classifier</li>
</ol>

<p><em>Note: There are other definitions of importance, however in this tutorial we limit our discussion to gini importance.</em></p>

<h2 id="preliminaries">Preliminaries</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-weight:bold">import</span> <span style="color:#0e84b5;font-weight:bold">numpy</span> <span style="color:#080;font-weight:bold">as</span> <span style="color:#0e84b5;font-weight:bold">np</span>
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">sklearn.ensemble</span> <span style="color:#080;font-weight:bold">import</span> RandomForestClassifier
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">sklearn</span> <span style="color:#080;font-weight:bold">import</span> datasets
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">sklearn.model_selection</span> <span style="color:#080;font-weight:bold">import</span> train_test_split
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">sklearn.feature_selection</span> <span style="color:#080;font-weight:bold">import</span> SelectFromModel
<span style="color:#080;font-weight:bold">from</span> <span style="color:#0e84b5;font-weight:bold">sklearn.metrics</span> <span style="color:#080;font-weight:bold">import</span> accuracy_score</code></pre></div>
<h2 id="create-the-data">Create The Data</h2>

<p>The dataset used in this tutorial is the famous <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">iris dataset</a>. The Iris target data contains 50 samples from three species of Iris, <code>y</code> and four feature variables, <code>X</code>.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Load the iris dataset</span>
iris <span style="color:#333">=</span> datasets<span style="color:#333">.</span>load_iris()

<span style="color:#888"># Create a list of feature names</span>
feat_labels <span style="color:#333">=</span> [<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;Sepal Length&#39;</span>,<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;Sepal Width&#39;</span>,<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;Petal Length&#39;</span>,<span style="background-color:#fff0f0"></span><span style="background-color:#fff0f0">&#39;Petal Width&#39;</span>]

<span style="color:#888"># Create X from the features</span>
X <span style="color:#333">=</span> iris<span style="color:#333">.</span>data

<span style="color:#888"># Create y from output</span>
y <span style="color:#333">=</span> iris<span style="color:#333">.</span>target</code></pre></div>
<h2 id="view-the-data">View The Data</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># View the features</span>
X[<span style="color:#00d;font-weight:bold">0</span>:<span style="color:#00d;font-weight:bold">5</span>]</code></pre></div>
<pre><code>array([[ 5.1,  3.5,  1.4,  0.2],
       [ 4.9,  3. ,  1.4,  0.2],
       [ 4.7,  3.2,  1.3,  0.2],
       [ 4.6,  3.1,  1.5,  0.2],
       [ 5. ,  3.6,  1.4,  0.2]])
</code></pre>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># View the target data</span>
y</code></pre></div>
<pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
</code></pre>

<h2 id="split-the-data-into-training-and-test-sets">Split The Data Into Training And Test Sets</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Split the data into 40% test and 60% training</span>
X_train, X_test, y_train, y_test <span style="color:#333">=</span> train_test_split(X, y, test_size<span style="color:#333">=</span><span style="color:#60e;font-weight:bold">0.4</span>, random_state<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">0</span>)</code></pre></div>
<h2 id="train-a-random-forest-classifier">Train A Random Forest Classifier</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Create a random forest classifier</span>
clf <span style="color:#333">=</span> RandomForestClassifier(n_estimators<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">10000</span>, random_state<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">0</span>, n_jobs<span style="color:#333">=-</span><span style="color:#00d;font-weight:bold">1</span>)

<span style="color:#888"># Train the classifier</span>
clf<span style="color:#333">.</span>fit(X_train, y_train)

<span style="color:#888"># Print the name and gini importance of each feature</span>
<span style="color:#080;font-weight:bold">for</span> feature <span style="color:#000;font-weight:bold">in</span> <span style="color:#007020">zip</span>(feat_labels, clf<span style="color:#333">.</span>feature_importances_):
    <span style="color:#080;font-weight:bold">print</span>(feature)</code></pre></div>
<pre><code>('Sepal Length', 0.11024282328064565)
('Sepal Width', 0.016255033655398394)
('Petal Length', 0.45028123999239533)
('Petal Width', 0.42322090307156124)
</code></pre>

<p>The scores above are the importance scores for each variable. There are two things to note. First, all the importance scores add up to 100%. Second, <code>Petal Length</code> and <code>Petal Width</code> are far more important than the other two features. Combined, <code>Petal Length</code> and <code>Petal Width</code> have an importance of ~0.86! Clearly these are the most importance features.</p>

<h2 id="identify-and-select-most-important-features">Identify And Select Most Important Features</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Create a selector object that will use the random forest classifier to identify</span>
<span style="color:#888"># features that have an importance of more than 0.15</span>
sfm <span style="color:#333">=</span> SelectFromModel(clf, threshold<span style="color:#333">=</span><span style="color:#60e;font-weight:bold">0.15</span>)

<span style="color:#888"># Train the selector</span>
sfm<span style="color:#333">.</span>fit(X_train, y_train)</code></pre></div>
<pre><code>SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=10000, n_jobs=-1, oob_score=False, random_state=0,
            verbose=0, warm_start=False),
        prefit=False, threshold=0.15)
</code></pre>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Print the names of the most important features</span>
<span style="color:#080;font-weight:bold">for</span> feature_list_index <span style="color:#000;font-weight:bold">in</span> sfm<span style="color:#333">.</span>get_support(indices<span style="color:#333">=</span>True):
    <span style="color:#080;font-weight:bold">print</span>(feat_labels[feature_list_index])</code></pre></div>
<pre><code>Petal Length
Petal Width
</code></pre>

<h2 id="create-a-data-subset-with-only-the-most-important-features">Create A Data Subset With Only The Most Important Features</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Transform the data to create a new dataset containing only the most important features</span>
<span style="color:#888"># Note: We have to apply the transform to both the training X and test X data.</span>
X_important_train <span style="color:#333">=</span> sfm<span style="color:#333">.</span>transform(X_train)
X_important_test <span style="color:#333">=</span> sfm<span style="color:#333">.</span>transform(X_test)</code></pre></div>
<h2 id="train-a-new-random-forest-classifier-using-only-most-important-features">Train A New Random Forest Classifier Using Only Most Important Features</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Create a new random forest classifier for the most important features</span>
clf_important <span style="color:#333">=</span> RandomForestClassifier(n_estimators<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">10000</span>, random_state<span style="color:#333">=</span><span style="color:#00d;font-weight:bold">0</span>, n_jobs<span style="color:#333">=-</span><span style="color:#00d;font-weight:bold">1</span>)

<span style="color:#888"># Train the new classifier on the new dataset containing the most important features</span>
clf_important<span style="color:#333">.</span>fit(X_important_train, y_train)</code></pre></div>
<pre><code>RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=10000, n_jobs=-1, oob_score=False, random_state=0,
            verbose=0, warm_start=False)
</code></pre>

<h2 id="compare-the-accuracy-of-our-full-feature-classifier-to-our-limited-feature-classifier">Compare The Accuracy Of Our Full Feature Classifier To Our Limited Feature Classifier</h2>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Apply The Full Featured Classifier To The Test Data</span>
y_pred <span style="color:#333">=</span> clf<span style="color:#333">.</span>predict(X_test)

<span style="color:#888"># View The Accuracy Of Our Full Feature (4 Features) Model</span>
accuracy_score(y_test, y_pred)</code></pre></div>
<pre><code>0.93333333333333335
</code></pre>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#888"># Apply The Full Featured Classifier To The Test Data</span>
y_important_pred <span style="color:#333">=</span> clf_important<span style="color:#333">.</span>predict(X_important_test)

<span style="color:#888"># View The Accuracy Of Our Limited Feature (2 Features) Model</span>
accuracy_score(y_test, y_important_pred)</code></pre></div>
<pre><code>0.8833333333333333
</code></pre>

<p>As can be seen by the accuracy scores, our original model which contained all four features is 93.3% accurate while the our &lsquo;limited&rsquo; model which contained only two features is 88.3% accurate. Thus, for a small cost in accuracy we halved the number of features in the model.</p>


</article> 



        </div> <!-- /.blog-main -->

      </div> <!-- /.row -->
    </div> <!-- /.container -->

    <footer class="blog-footer">
      <p>
      
      Blog template created by <a href="https://twitter.com/mdo">@mdo</a>, ported to Hugo by <a href='https://twitter.com/mralanorth'>@mralanorth</a>.
      
      </p>
    </footer>

  </body>

</html>
